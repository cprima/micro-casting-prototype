# Sitemap Crawler Roadmap

Version-based feature roadmap showing implemented vs planned features.

---

## Current Status: v0.1.0 âœ… COMPLETE

**Release Date**: 2025-01-22

### âœ… Implemented Features

**Core Functionality:**
- âœ… Basic app structure under `apps/sitemap-crawler/`
- âœ… llms.txt parser with markdown link support
- âœ… XML sitemap parser with recursive sitemap index support
- âœ… Local filesystem storage with `BaseStorage` interface
- âœ… Integration with `crawling` library (Crawl4AI) for markdown conversion
- âœ… Synchronous crawling (one URL at a time)

**CLI Commands:**
- âœ… `sitemap-crawler list` - Show configured sites
- âœ… `sitemap-crawler crawl <name>` - Crawl single site
- âœ… `sitemap-crawler crawl-all` - Crawl all sites
- âœ… `--dry-run` - Preview URLs without crawling
- âœ… `--verbose` - Enable debug logging

**Configuration:**
- âœ… config.yaml with 2 example sites (modelcontextprotocol.io, docs.uipath.com)
- âœ… URL filtering (pattern matching, contains)
- âœ… Per-site base_dir configuration (different output locations per site)
- âœ… Environment variable expansion (`${VAR}`)
- âœ… Output path templates (`{domain}`, `{date}`)

**File Handling:**
- âœ… File naming safety (sanitize special characters)
- âœ… Cross-platform path support (Windows/Linux/Mac)
- âœ… Automatic parent directory creation

**Logging:**
- âœ… Basic console logging
- âœ… Detailed filter logging (shows before/after counts)
- âœ… Sitemap index vs regular sitemap differentiation
- âœ… Per-URL crawl progress

**Documentation:**
- âœ… Comprehensive README.md
- âœ… Example config.yaml with comments
- âœ… ROADMAP.md (this file)
- âœ… CC-BY 4.0 LICENSE

### ğŸ¯ Success Criteria Met
- âœ… Successfully crawl https://modelcontextprotocol.io/llms.txt (43 URLs)
- âœ… Successfully crawl English URLs from https://docs.uipath.com/sitemap.xml (~32K URLs)
- âœ… Output markdown files to configurable directories
- âœ… Per-site storage configuration working

---

## Next: v0.2.0 - Production Ready âœ… COMPLETE

**Goal**: Add resilience, observability, and production-grade features

**Completed**: 2025-10-22

### ğŸš§ Recommended Priority Order

#### **Phase 1: Best-in-Class Logging** ğŸ“Š âœ… COMPLETE

**Why First**: Essential for debugging large crawls, understanding performance bottlenecks

- âœ… **Structured logging with `structlog`**
  - JSON output format for machine parsing
  - Human-readable console output for development
  - Correlation IDs to trace requests across components
  - Context binding (site name, URL, crawl session)

- âœ… **Performance metrics**
  - Request duration tracking
  - Bytes downloaded per request
  - Average speed (URLs/second, MB/second)
  - Crawl session metrics with summary

- âœ… **Log levels properly implemented**
  - DEBUG: Filter details, parser internals, HTTP requests
  - INFO: Crawl progress, major operations, filtering results
  - WARNING: Skipped URLs, retries
  - ERROR: Failed requests, exceptions with stack traces
  - Configurable via `--log-level` CLI option

- âœ… **Configurable log outputs**
  - Console (human-readable with colors via structlog.dev.ConsoleRenderer)
  - File (JSON lines for log aggregation at logs/sitemap-crawler.log)
  - Rotation policy (10MB files, 5 backups)
  - Configurable via `--log-file` CLI option

#### **Phase 2: Progress Tracking** ğŸ“ˆ âœ… COMPLETE

**Why Next**: User feedback for long-running crawls

- âœ… **Progress bar with `tqdm`**
  - Visual progress for URL fetching
  - Nested progress bars (sites â†’ URLs)
  - ETA calculation
  - Speed indicator (URLs/s)

- âœ… **Crawl statistics**
  - Real-time counters (success/fail/skipped)
  - Total bytes downloaded
  - Total duration
  - Summary report at end

#### **Phase 3: Resilience & Politeness** ğŸ›¡ï¸ âœ… COMPLETE

**Why Next**: Required for production use on real sites

- âœ… **Retry logic with exponential backoff**
  - Configurable max retries (default: 3)
  - Exponential backoff (1s, 2s, 4s, 8s...)
  - Retry on network errors, HTTP 5xx
  - Don't retry on HTTP 4xx (client errors)

- âœ… **Rate limiting**
  - Configurable delay between requests (default: 1s)
  - Per-domain rate limiting
  - Respect HTTP 429 (Too Many Requests)
  - Adaptive rate limiting

- âœ… **Custom User-Agent**
  - Configurable User-Agent string
  - Default: `sitemap-crawler/0.2.0 (+https://github.com/...)`
  - Per-site override

- âœ… **Timeouts from config**
  - Request timeout (default: 30s)
  - Connection timeout (default: 10s)
  - Per-site override

- âŒ **Config reload on signal (SIGHUP)** (Deferred to v0.5.0)
  - Reload config.yaml without stopping crawl
  - Update settings for queued URLs (not in-progress)
  - Useful for long-running crawls with many sites
  - Handle config errors gracefully (keep old config on error)

#### **Phase 4: Resource Limits** âš™ï¸ âœ… COMPLETE

**Why Next**: Safety constraints for production

- âœ… **Configurable limits**
  - Max URLs per site (default: 50000)
  - Max file size (default: 10MB, skip larger)
  - Max total size per crawl (default: 5000MB)
  - Max crawl duration (timeout, default: unlimited)

- âœ… **Content validation**
  - Empty page detection (skip if < 100 chars)
  - Minimum content threshold (configurable)
  - HTML error page detection (404/500 saved as HTML)

#### **Phase 5: Enhanced File Handling** ğŸ“ âœ… COMPLETE

- âœ… **Collision detection**
  - Detect duplicate filenames
  - Append counter: `page.md`, `page-1.md`, `page-2.md`
  - Log collisions

- âœ… **Improved path sanitization**
  - Handle extremely long URLs (> 260 chars on Windows)
  - Hash-based filenames for very long URLs (MD5 hash truncated to 8 chars)
  - Preserve URL structure option

### âŒ Not Implemented (Deferred to v0.5.0)

- âŒ SMBStorage implementation (BaseStorage interface exists, implementation deferred)
- âŒ Config reload on signal (SIGHUP)

### ğŸ“‹ Success Criteria for v0.2.0

- âœ… Crawl 1000+ URLs from docs.uipath.com with retry logic
- âœ… Handle network failures gracefully (exponential backoff)
- âœ… JSON-structured logs available for analysis
- âœ… Progress bar shows ETA and speed
- âœ… Rate limiting prevents server overload
- âœ… All logs include correlation IDs for tracing
- âœ… Resource limits enforced (max URLs, file size, content validation)
- âœ… Filename collision detection working

---

## v0.2.1 - robots.txt Compliance âœ… COMPLETE

**Goal**: Respect website robots.txt policies (from v0.5.0 roadmap, implemented early)

**Completed**: 2025-10-23

### âœ… Implemented Features

**robots.txt Compliance:**
- âœ… Automatic robots.txt fetching per domain
- âœ… URL filtering based on disallow rules
- âœ… crawl-delay directive support (overrides configured rate limit)
- âœ… Per-domain caching (1 hour default, configurable)
- âœ… Configurable enable/disable via config.yaml
- âœ… Graceful fallback on fetch errors (allows by default)
- âœ… Proper User-Agent identification

### ğŸ“‹ Success Criteria for v0.2.1

- âœ… Fetch and parse robots.txt successfully
- âœ… Respect disallow directives
- âœ… Honor crawl-delay when specified
- âœ… Cache robots.txt to avoid repeated fetches
- âœ… Gracefully handle missing robots.txt (404)

---

## v0.3.0 - Architecture Cleanup âœ… COMPLETE

**Release Date**: 2025-10-23

**Goal**: Clean separation of concerns between libs/crawling and apps/sitemap-crawler

**Status**: Complete

### âœ… Implemented Features

**libs/crawling Refactoring:**
- âœ… Minimal wrapper library (pinning crawl4ai==0.7.6 only)
- âœ… Re-exports crawl4ai types (BrowserConfig, CrawlerRunConfig, CacheMode, CrawlResult)
- âœ… Returns full CrawlResult (not just markdown string)
- âœ… No hardcoded configuration decisions
- âœ… Updated documentation reflecting Unix philosophy: "Do one thing well"

**apps/sitemap-crawler Configuration:**
- âœ… Global `browser:` settings in config.yaml (headless, viewport, user_agent)
- âœ… Global `crawl_defaults:` settings with pruning configuration
- âœ… Per-site `crawl4ai:` overrides continue to work
- âœ… Config class builds BrowserConfig and CrawlerRunConfig from YAML

**Future-Proofing:**
- âœ… Access to response_headers (enables Last-Modified/ETag for incremental updates)
- âœ… Access to status_code and full CrawlResult metadata
- âœ… Clean foundation for v0.5.0 advanced features

### ğŸ¯ Success Criteria Met
- âœ… Clean separation: libs = pinning only, apps = all configuration
- âœ… No hardcoded browser/run config in library layer
- âœ… Existing crawls work unchanged with new architecture
- âœ… Full CrawlResult available for future features

---

## v0.4.0 - Distributed Work Queue ğŸ’­ PLANNED

**Goal**: Filesystem-based distributed crawling with resumable state and incremental updates

**Status**: Design phase

### Problem Statement

**Current Limitations:**
- No persistent state â†’ cannot resume failed crawls
- No work distribution â†’ single process only
- No incremental updates â†’ re-crawls everything
- Doesn't scale for large sites (34K+ URLs like UIPath)

**Why Not SQLite:**
- Single writer bottleneck defeats concurrency
- Network filesystem corruption risk (SMB/NFS + SQLite = bad)
- Lock contention when multiple workers compete

### Core Architecture

**State lives in output directory:**
```
{base_output_dir}/.crawl-state/{site_name}/
â”œâ”€â”€ tickets/
â”‚   â”œâ”€â”€ pending/
â”‚   â”‚   â”œâ”€â”€ 0001.json    # URLs 0-99
â”‚   â”‚   â”œâ”€â”€ 0002.json    # URLs 100-199
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ claimed/
â”‚   â”‚   â””â”€â”€ 0003.json    # Worker-1 processing
â”‚   â”œâ”€â”€ completed/
â”‚   â”‚   â””â”€â”€ 0001.json    # Done
â”‚   â””â”€â”€ failed/
â”‚       â””â”€â”€ 0042.json    # Failed after retries
â”œâ”€â”€ manifest.json        # Crawl metadata
â”œâ”€â”€ url-index.json       # URL â†’ {last_modified, etag, hash, status}
â””â”€â”€ workers/
    â”œâ”€â”€ worker-1.lock    # Heartbeat file
    â””â”€â”€ worker-2.lock
```

### Features

**Phase 1: Ticket-Based Work Distribution**
- [ ] Split URL list into tickets (default: 100 URLs per ticket)
- [ ] Atomic ticket claiming via filesystem rename()
- [ ] Worker heartbeat mechanism (detect dead workers)
- [ ] Automatic stale ticket recovery
- [ ] Progress tracking per ticket

**Phase 2: Incremental Updates**
- [ ] URL index tracking (Last-Modified, ETag, content hash)
- [ ] Conditional HTTP requests (If-Modified-Since, If-None-Match)
- [ ] Skip unchanged content (HTTP 304 or matching hash)
- [ ] Detect removed URLs (delete from output)
- [ ] Access to CrawlResult.response_headers (implemented in v0.3.0)

**Phase 3: Distributed Workers**
- [ ] Multi-process workers on same machine
- [ ] Multi-machine workers (shared network filesystem)
- [ ] No central coordinator required
- [ ] Automatic load balancing via ticket claiming

### Ticket Format

```json
{
  "ticket_id": "0001",
  "site": "uipath-docs",
  "created_at": "2025-10-23T12:00:00Z",
  "urls": [
    {
      "url": "https://docs.uipath.com/page1",
      "last_seen": "2025-10-23T12:00:00Z"
    }
  ],
  "size": 100,
  "claimed_by": null,
  "claimed_at": null,
  "completed_at": null,
  "progress": {
    "total": 100,
    "success": 0,
    "failed": 0,
    "skipped": 0
  }
}
```

### URL Index Format

```json
{
  "https://docs.uipath.com/page1": {
    "last_modified": "2025-10-20T10:00:00Z",
    "etag": "\"abc123\"",
    "content_hash": "sha256:...",
    "status_code": 200,
    "last_crawled": "2025-10-23T12:00:00Z",
    "file_path": "docs/uipath.com/en/page1.md"
  }
}
```

### Atomic Operations

**Ticket State Transitions (using filesystem rename):**
```
pending/0001.json â†’ claimed/0001.json    # Atomic claim
claimed/0001.json â†’ completed/0001.json  # Atomic complete
claimed/0001.json â†’ failed/0001.json     # Atomic fail
```

**Worker Coordination:**
- Heartbeat files updated every 5 seconds
- Workers declared dead after 60s without heartbeat
- Stale tickets automatically reclaimed to pending/

### CLI Commands

```bash
# Prepare tickets for a site
sitemap-crawler prepare uipath-docs

# Work on tickets (single worker)
sitemap-crawler work uipath-docs

# Work with multiple processes
sitemap-crawler work uipath-docs --workers 4

# Resume failed crawl
sitemap-crawler resume uipath-docs

# Show progress
sitemap-crawler status uipath-docs
```

### Configuration

```yaml
settings:
  crawl_state:
    enabled: true                    # Enable ticket system
    ticket_size: 100                 # URLs per ticket
    heartbeat_interval: 5            # Seconds
    heartbeat_timeout: 60            # Declare worker dead after 60s
    incremental: true                # Enable incremental updates

sites:
  - name: uipath-docs
    source: https://docs.uipath.com/sitemap.xml
    crawl_state:
      ticket_size: 500               # Override for large sites
```

### Benefits

âœ… **Distributed**: Multiple workers, multiple machines
âœ… **Resumable**: Pickup where left off after crash
âœ… **Incremental**: Only crawl changed content
âœ… **Observable**: Clear progress via ticket status
âœ… **No Database**: Pure filesystem, works on SMB/NFS
âœ… **Atomic**: Filesystem rename provides coordination
âœ… **Scalable**: 34K URLs â†’ 340 tickets (@ 100 URLs/ticket)
âœ… **State with Data**: Travels with output directory

### Success Criteria

- [ ] Prepare 34K UIPath URLs into ~340 tickets
- [ ] 4 workers claim tickets concurrently without conflicts
- [ ] Kill worker-2 mid-crawl, tickets reclaimed automatically
- [ ] Resume crawl after crash (only pending tickets re-crawled)
- [ ] Incremental re-crawl skips unchanged URLs (via Last-Modified/ETag)
- [ ] Network filesystem test (SMB mount, multiple machines)

---

## Backlog: v1.9.0 - Future Ideas ğŸ”®

**Status**: Ideas only, not committed

**Note**: Incremental updates and distributed crawling moved to v0.4.0

### Async/Concurrent Crawling (within single process)
- Async implementation using `asyncio` + crawl4ai's arun_many()
- Concurrent URL fetching (configurable max workers)
- Connection pooling (reuse HTTP connections)
- 5-10x performance improvement for large crawls

### Advanced Sitemap Support
- Compressed sitemaps (.xml.gz)
- RSS/Atom feed parsing
- Custom sitemap formats (extensible parser system)

### Crawl Manifest
- JSON manifest per crawl: `manifest.json`
- Contains: timestamp, URL list, status codes, hashes
- Enable auditing and debugging

### Enhanced Filtering
- Regex pattern matching
- Multiple filter types (AND/OR logic)
- Domain whitelist/blacklist
- Content-type filtering

### Additional Storage Backends
- SMBStorage implementation (smbprotocol)
- S3/MinIO storage backend
- WebDAV storage backend

### Observability
- Prometheus metrics export
- Crawl history database (past runs)
- Error report generation

### Post-processing & Integration
- Post-crawl hooks (run arbitrary scripts)
- Index generation (SUMMARY.md, index.json)
- Vector embedding preparation
- Full-text search index (Elasticsearch/Meilisearch)

### Advanced Content Extraction
- CSS/XPath selectors for specific content sections
- Readability mode (extract main content only)
- Table extraction and conversion
- Image downloading and local storage

### Scheduling & Automation
- Built-in scheduler (cron-like)
- Watch mode (re-crawl on sitemap changes)
- Webhook triggers
- GitHub Actions integration

### Authentication
- HTTP Basic/Digest auth support
- OAuth2 flow support
- Cookie-based session handling
- API key authentication

### UI/Dashboard
- Web UI for monitoring crawls
- Real-time progress dashboard
- Crawl configuration editor
- Historical analytics

### Content Transformation
- Pandoc integration for multiple output formats
- HTML â†’ PDF conversion
- Archive format support (.warc, .har)

### Notification System
- Email notifications on completion
- Slack/Discord webhooks
- Error alerting

---

## Version History

| Version | Status | Date | Notes |
|---------|--------|------|-------|
| 0.1.0 | âœ… **Released** | 2025-01-22 | Initial prototype with basic crawling |
| 0.2.0 | âœ… **Released** | 2025-10-22 | Production-ready features (logging, retry, limits, progress, validation) |
| 0.2.1 | âœ… **Released** | 2025-10-23 | robots.txt compliance (from v0.5.0 roadmap) |
| 0.3.0 | âœ… **Released** | 2025-10-23 | Architecture cleanup (libs/crawling minimal wrapper, config separation) |
| 0.4.0 | ğŸ’­ Planned | TBD | Distributed work queue (filesystem-based tickets, incremental updates) |
| 1.9.0 | ğŸ”® Backlog | TBD | Future ideas |

---

## Immediate Next Steps (Recommended Order)

1. âœ… ~~**Implement structured logging with `structlog`**~~ (COMPLETE)
   - âœ… JSON output format
   - âœ… Correlation IDs
   - âœ… Performance metrics
   - âœ… Multiple output targets (console + file)

2. âœ… ~~**Add progress tracking with `tqdm`**~~ (COMPLETE)
   - âœ… Visual progress bars
   - âœ… Speed indicators
   - âœ… ETA calculation

3. âœ… ~~**Implement retry logic**~~ (COMPLETE)
   - âœ… Exponential backoff
   - âœ… Configurable max retries
   - âœ… Smart retry (only on network/5xx errors)

4. âœ… ~~**Add rate limiting**~~ (COMPLETE)
   - âœ… Configurable delays
   - âœ… Per-domain limits
   - âœ… HTTP 429 handling

5. âœ… ~~**Resource limits from config**~~ (COMPLETE)
   - âœ… Max URLs, file size, duration
   - âœ… Content validation
   - âœ… Graceful limit handling
   - âœ… Filename collision detection
   - âœ… Long URL handling

**Next: v0.4.0 - Distributed Work Queue**
- Filesystem-based ticket system for work distribution
- Incremental updates via Last-Modified/ETag headers
- Multi-worker support (multi-process, multi-machine)
- Resumable crawls after crashes
- State lives with data in output directory

---

## Contributing

Features are prioritized based on:
1. **User demand** - What users actually need
2. **Implementation complexity** - Low-hanging fruit first
3. **Dependencies** - Foundation features before advanced ones
4. **Maintenance burden** - Sustainable long-term

To propose a feature:
1. Check if it's already in the backlog
2. Open an issue describing the use case
3. Discuss implementation approach
4. Submit PR if approved

---

## Author

Christian Prior-Mamulyan (cprior@gmail.com)

## License

CC-BY 4.0 - See [LICENSE](../../LICENSE)
